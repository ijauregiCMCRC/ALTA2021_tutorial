{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Translation notebook\n",
    "\n",
    "This is the notebook for translation\n",
    "\n",
    "(More descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TestTubeLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# from pytorch_lightning.overrides.data_parallel import LightningDistributedDataParallel\n",
    "\n",
    "\n",
    "from src.translation_lightning_model import LmForTranslation\n",
    "from src.translation_metrics import translation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument(\"--train_data\", type=str, required=True, help='Path to training data')\n",
    "parser.add_argument(\"--validation_data\", type=str, required=True, help='Path to validation data')\n",
    "parser.add_argument(\"--test_data\", type=str, required=True, help='Path to testing data')\n",
    "parser.add_argument(\"--src\", type=str, required=True, help='Source language.')\n",
    "parser.add_argument(\"--tgt\", type=str, required=True, help='Target language.')\n",
    "parser.add_argument(\"--save_dir\", type=str, default='translation')\n",
    "parser.add_argument(\"--save_prefix\", type=str, default='test')\n",
    "parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size\")\n",
    "parser.add_argument(\"--grad_accum\", type=int, default=1, help=\"number of gradient accumulation steps\")\n",
    "parser.add_argument(\"--max_grad_norm\", type=float, default=1.0, help=\"number of gradient accumulation steps\")\n",
    "parser.add_argument(\"--gpus\", type=int, default=0,\n",
    "                    help=\"Number of gpus. 0 for CPU\")\n",
    "parser.add_argument(\"--warmup\", type=int, default=500, help=\"Number of warmup steps\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.00003, help=\"Maximum learning rate\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0.01, help=\"Adam weight decay\")\n",
    "parser.add_argument(\"--val_every\", type=float, default=1.0, help=\"Number of training steps between validations\")\n",
    "parser.add_argument(\"--val_percent_check\", default=1.00, type=float, help='Percent of validation data used')\n",
    "parser.add_argument(\"--num_workers\", type=int, default=0, help=\"Number of data loader workers\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1234, help=\"Seed\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=1, help=\"Number of epochs\")\n",
    "parser.add_argument(\"--disable_checkpointing\", action='store_true', help=\"No logging or checkpointing\")\n",
    "parser.add_argument(\"--max_input_len\", type=int, default=170,\n",
    "                    help=\"maximum num of wordpieces/summary. Used for training and testing\")\n",
    "parser.add_argument(\"--max_output_len\", type=int, default=170,\n",
    "                    help=\"maximum num of wordpieces/summary. Used for training and testing\")\n",
    "parser.add_argument(\"--test\", action='store_true', help=\"Test only, no training\")\n",
    "parser.add_argument(\"--model_lm_path\", type=str, default='../pretrained_lms/sshleifer-tiny-mbart',\n",
    "                    help=\"Path to the checkpoint directory or model name\")\n",
    "parser.add_argument(\"--tokenizer\", type=str, default='../pretrained_lms/sshleifer-tiny-mbart')\n",
    "parser.add_argument(\"--add_adapter\", action='store_true', help=\"Add an adapter.\")\n",
    "parser.add_argument(\"--progress_bar\", type=int, default=10, help=\"Progress bar. Good for printing\")\n",
    "parser.add_argument(\"--precision\", type=int, default=32, help=\"Double precision (64), full precision (32) \"\n",
    "                                                              \"or half precision (16). Can be used on CPU, \"\n",
    "                                                              \"GPU or TPUs.\")\n",
    "parser.add_argument(\"--amp_backend\", type=str, default='native', help=\"The mixed precision backend to \"\n",
    "                                                                      \"use ('native' or 'apex')\")\n",
    "parser.add_argument(\"--debug\", action='store_true', help=\"debug run\")\n",
    "parser.add_argument(\"--resume_ckpt\", type=str, help=\"Path of a checkpoint to resume from\")\n",
    "parser.add_argument(\"--from_pretrained\", type=str, default=None,\n",
    "                    help=\"Path to a checkpoint to load model weights but not training state\")\n",
    "parser.add_argument('--grad_ckpt', action='store_true', help='Enable gradient checkpointing to save memory')\n",
    "parser.add_argument(\"--attention_dropout\", type=float, default=0.1, help=\"attention dropout\")\n",
    "parser.add_argument(\"--attention_mode\", type=str, default='sliding_chunks', help=\"Longformer attention mode\")\n",
    "parser.add_argument(\"--attention_window\", type=int, default=512, help=\"Attention window\")\n",
    "parser.add_argument(\"--label_smoothing\", type=float, default=0.0, required=False)\n",
    "parser.add_argument(\"--adafactor\", action='store_true', help=\"Use adafactor optimizer\")\n",
    "\n",
    "\n",
    "args ={\n",
    "    'train_data': '',\n",
    "    'validation_data': '',\n",
    "    'test_data': '',\n",
    "    'src': '',\n",
    "    'tgt': '',\n",
    "    'save_dir': '',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "model = LmForTranslation(args)\n",
    "\n",
    "model.hf_datasets = {'train': args.train_data,\n",
    "                     'validation': args.validation_data,\n",
    "                     'test': args.test_data}\n",
    "print(model.hf_datasets)\n",
    "\n",
    "logger = TestTubeLogger(\n",
    "    save_dir=args.save_dir,\n",
    "    name=args.save_prefix,\n",
    "    version=0  # always use version=0\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(args.save_dir, args.save_prefix, \"checkpoints\"),\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='BLEU',\n",
    "    mode='max',\n",
    "    period=0\n",
    ")\n",
    "\n",
    "print(args)\n",
    "\n",
    "args.dataset_size = 203037  # hardcode dataset size. Needed to compute number of steps for the lr scheduler\n",
    "\n",
    "trainer = pl.Trainer(gpus=args.gpus, distributed_backend='ddp' if torch.cuda.is_available() else None,\n",
    "                     track_grad_norm=-1,\n",
    "                     max_epochs=args.epochs if not args.debug else 100,\n",
    "                     max_steps=None if not args.debug else 1,\n",
    "                     replace_sampler_ddp=False,\n",
    "                     accumulate_grad_batches=args.grad_accum,\n",
    "                     gradient_clip_val=args.max_grad_norm,\n",
    "                     val_check_interval=args.val_every if not args.debug else 1,\n",
    "                     num_sanity_val_steps=2 if not args.debug else 0,\n",
    "                     check_val_every_n_epoch=1 if not args.debug else 1,\n",
    "                     logger=logger,\n",
    "                     callbacks=checkpoint_callback if not args.disable_checkpointing else False,\n",
    "                     progress_bar_refresh_rate=args.progress_bar,\n",
    "                     precision=args.precision,\n",
    "                     amp_backend=args.amp_backend, amp_level='O2',\n",
    "                     resume_from_checkpoint=args.resume_ckpt,\n",
    "                     )\n",
    "if not args.test:\n",
    "    trainer.fit(model)\n",
    "trainer.test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
