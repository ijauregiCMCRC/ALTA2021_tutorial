{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Translation notebook\n",
    "\n",
    "This is the notebook for translation\n",
    "\n",
    "(More descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Only for  Google Colab Execution)\n",
    "\n",
    "If you are running the notebook in Google Colab, run the cell below to download the repository witht he required files to run the models and the requirements file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ijauregiCMCRC/ALTA2021_tutorial.git\n",
    "%cd /ALTA2021_tutorial/translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TestTubeLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import gdown\n",
    "\n",
    "from src.translation_lightning_model import LmForTranslation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args ={\n",
    "    'train_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/train_pr',  # Path to training data\n",
    "    'validation_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/dev2010',  # Path to validation data\n",
    "    'test_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/test_joined',  # Path to test data\n",
    "    'src': 'en',  # Source language prefix\n",
    "    'tgt': 'es',  # Target language prefix\n",
    "    'max_src_len': 170,  # Maximum number of tokens in the source sentence\n",
    "    'max_tgt_len': 170,  # Maximum number of tokens in the target sentence\n",
    "    'save_dir': '../models/iwslt_2014/es-en/mBART_large_pretrained_mt_plus_adapter',  # Path to save the model and logs\n",
    "    'tokenizer': 'facebook/mbart-large-cc25',  # Pretrained tokenizer\n",
    "    'model': 'mrm8488/mbart-large-finetuned-opus-en-es-translation',  # Pretrained model\n",
    "    'add_adapter': False,  # Include adapter training\n",
    "    'reduction_factor': 1,  # Adapter's reduction factor (>= 1)\n",
    "    'label_smoothing': 0.1, # Label smoothing \n",
    "    'epochs': 1,  # Number of epochs during training\n",
    "    'batch_size': 8,  # Batch size\n",
    "    'grad_accum': 1,  # Gradient accumulation\n",
    "    'lr': 0.00003,  # Training learning rate\n",
    "    'warmup': 500,  # Number of warmup steps\n",
    "    'weight_decay': 0.00003,  # Adam weight decay\n",
    "    'gpus': 1,  # Number of gpus. 0 for CPU\n",
    "    'precision': 32,  # Double precision (64), full precision (32) \n",
    "                      # or half precision (16). Can be used on CPU, GPU or TPUs.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ../models/iwslt_2014/es-en/mBART_large_pretrained_mt_plus_adapter/training/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:488: LightningDeprecationWarning: Argument `period` in `ModelCheckpoint` is deprecated in v1.3 and will be removed in v1.5. Please use `every_n_epochs` instead.\n",
      "  \"Argument `period` in `ModelCheckpoint` is deprecated in v1.3 and will be removed in v1.5.\"\n",
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:111: LightningDeprecationWarning: `Trainer(distributed_backend=dp)` has been deprecated and will be removed in v1.5. Use `Trainer(accelerator=dp)` instead.\n",
      "  f\"`Trainer(distributed_backend={distributed_backend})` has been deprecated and will be removed in v1.5.\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': './my_datasets/IWSLT_2014_TEDtalks/es-en/train_pr', 'validation': './my_datasets/IWSLT_2014_TEDtalks/es-en/dev2010', 'test': './my_datasets/IWSLT_2014_TEDtalks/es-en/test_joined'}\n",
      "{'train_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/train_pr', 'validation_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/dev2010', 'test_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/test_joined', 'src': 'en', 'tgt': 'es', 'max_src_len': 170, 'max_tgt_len': 170, 'save_dir': '../models/iwslt_2014/es-en/mBART_large_pretrained_mt_plus_adapter', 'tokenizer': 'facebook/mbart-large-cc25', 'model': 'mrm8488/mbart-large-finetuned-opus-en-es-translation', 'add_adapter': False, 'reduction_factor': 1, 'label_smoothing': 0.1, 'epochs': 1, 'batch_size': 8, 'grad_accum': 1, 'lr': 3e-05, 'warmup': 500, 'weight_decay': 3e-05, 'gpus': 1, 'precision': 32, 'dataset_size': 180850}\n"
     ]
    }
   ],
   "source": [
    "# Initialize with a seed\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# dataset size. Needed to compute number of steps for the lr scheduler\n",
    "args['dataset_size'] = sum(1 for line in open(args['train_data'] + '.' + args['src']))\n",
    "\n",
    "# Define PyTorch Lightning model\n",
    "model = LmForTranslation(args)\n",
    "# Include datasets\n",
    "#model.hf_datasets = {'train': args['train_data'],\n",
    "#                     'validation': args['validation_data'],\n",
    "#                     'test': args['test_data']}\n",
    "print(model.hf_datasets)\n",
    "\n",
    "# Define logger\n",
    "logger = TestTubeLogger(\n",
    "    save_dir=args['save_dir'],\n",
    "    name='training',\n",
    "    version=0  # always use version=0\n",
    ")\n",
    "\n",
    "# Define checkpoint saver\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(args['save_dir'], \"training\", \"checkpoints\"),  # Dir path\n",
    "    filename='check-{epoch:02d}-{BLEU:.2f}',  # Filename\n",
    "    save_top_k=1,  # Maximum number of checkpoints to be saved\n",
    "    verbose=True,  # Verbose\n",
    "    monitor='BLEU',  # Checkpointing measurement (BLEU validation)\n",
    "    mode='max',      # Maximize measurement over the validation\n",
    "    period=1         # Save every epoch\n",
    ")\n",
    "\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Define lightning trainer\n",
    "trainer = pl.Trainer(gpus=args['gpus'], distributed_backend='dp' if torch.cuda.is_available() else None,\n",
    "                     track_grad_norm=-1,\n",
    "                     max_epochs=args['epochs'],\n",
    "                     max_steps=None,\n",
    "                     replace_sampler_ddp=False,\n",
    "                     accumulate_grad_batches=args['grad_accum'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     val_check_interval=1.0,\n",
    "                     num_sanity_val_steps=2,\n",
    "                     check_val_every_n_epoch=1,\n",
    "                     logger=logger,\n",
    "                     callbacks=checkpoint_callback,\n",
    "                     progress_bar_refresh_rate=10,\n",
    "                     precision=args['precision'],\n",
    "                     amp_backend='native', amp_level='O2',\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | MBartForConditionalGeneration | 661 M \n",
      "--------------------------------------------------------\n",
      "50.4 M    Trainable params\n",
      "610 M     Non-trainable params\n",
      "661 M     Total params\n",
      "2,644.931 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/train_pr', 'validation_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/dev2010', 'test_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/test_joined', 'src': 'en', 'tgt': 'es', 'max_src_len': 170, 'max_tgt_len': 170, 'save_dir': '../models/iwslt_2014/es-en/mBART_large_pretrained_mt_plus_adapter', 'tokenizer': 'mrm8488/mbart-large-finetuned-opus-en-es-translation', 'model': 'mrm8488/mbart-large-finetuned-opus-en-es-translation', 'add_adapter': True, 'reduction_factor': 1, 'label_smoothing': 0.1, 'epochs': 1, 'batch_size': 8, 'grad_accum': 1, 'lr': 3e-05, 'warmup': 500, 'weight_decay': 3e-05, 'gpus': 1, 'precision': 32, 'dataset_size': 180850}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b5e411904d49518db794e3b01706ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 26 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 26 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6887def58b3b485ba6b10956cc9560e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 22594: BLEU reached 0.00000 (best 0.00000), saving model to \"/data/injaureg/Desktop/CMCRC/ALTA/ALTA2021_tutorial/models/iwslt_2014/es-en/mBART_large_pretrained_mt_plus_adapter/training/checkpoints/check-epoch=00-BLEU=0.00.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4490.29577255249\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Train model\n",
    "trainer.fit(model)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 26 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1432443c306c4ed9b8508b2692def757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_val_loss': tensor(2.5985, device='cuda:0'), 'avg_accuracy': tensor(0.3291, device='cuda:0'), 'log': {'vloss': tensor(2.5985, device='cuda:0'), 'vaccuracy': tensor(0.3291, device='cuda:0')}, 'progress_bar': {'vloss': tensor(2.5985, device='cuda:0'), 'vaccuracy': tensor(0.3291, device='cuda:0')}}\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BLEU': 29.53417205810547}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'BLEU': 29.53417205810547}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola, mi nombre es Inigo.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define PyTorch Lightning model\n",
    "model = LmForTranslation.load_from_checkpoint('../models/iwslt_2014/es-en/es_en_translation_bart_en_jupyter/'\n",
    "                                              'training/checkpoints/check-epoch=00-BLEU=30.43.ckpt')\n",
    "\n",
    "sentence = 'Hello my name is Inigo.'\n",
    "translation = model.translate_example(sentence)\n",
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create models folder\n",
    "!mkdir models\n",
    "# Download them from google drive\n",
    "# BART_base\n",
    "bart_base_url = 'https://drive.google.com/uc?id=1g3uNMlfEO6IsOxQ_KIVnAJ5E73jAe8cN'\n",
    "bart_base_out = './models/bart_base.zip'\n",
    "gdown.download(bart_base_url, bart_base_out, quiet=False)\n",
    "!unzip './models/bart_base_url.zip' -d './models/'\n",
    "!rm './models/bart_base_url.zip'\n",
    "# mBART_large\n",
    "mbart_large_url = 'https://drive.google.com/uc?id=1mHS7n7og00ZD3u9TD-CpyKefvYjgvPxn'\n",
    "mbart_large_out = './models/mbart_large.zip'\n",
    "gdown.download(mbart_large_url, mbart_large_out, quiet=False)\n",
    "!unzip './models/mbart_large.zip' -d './models/'\n",
    "!rm './models/mbart_large.zip'\n",
    "# mBART_large_with_adapter\n",
    "mbart_large_wa_url = 'https://drive.google.com/uc?id=1kTrcD-9J8XWP-jpSwuVPN8B94XtRCZVZ'\n",
    "mbart_large_wa_out = './models/mbart_large_wa.zip'\n",
    "gdown.download(mbart_large_wa_url, mbart_large_wa_out, quiet=False)\n",
    "!unzip './models/mbart_large_wa.zip' -d './models/'\n",
    "!rm './models/mbart_large_wa.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 26 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1052e526a6d54ac39b79a47d85db891f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BLEU': 20.49509048461914}\n",
      "--------------------------------------------------------------------------------\n",
      "BART_base:\n",
      "-----------------\n",
      "Test BLEU:  20.49509048461914\n",
      "Training time:  31.333333333333332  mins\n",
      "Inference time:  6.744376571973165  mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = LmForTranslation.load_from_checkpoint('models/BART_base/training/checkpoints/'\n",
    "                                              'check-epoch=00-BLEU=20.97.ckpt')\n",
    "\n",
    "start_time = time.time()\n",
    "test_bleu_bart_base = trainer.test(model)[0]['BLEU']\n",
    "training_time_bart_base = 1880\n",
    "inference_time_bart_base = time.time() - start_time\n",
    "print('BART_base:')\n",
    "print('-----------------')\n",
    "print('Test BLEU: ', test_bleu_bart_base)\n",
    "print('Training time: ', training_time_bart_base / 60, ' mins')\n",
    "print('Inference time: ', inference_time_bart_base / 60, ' mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 26 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778cc3b17432497382244ee5031b22cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BLEU': 34.681053161621094}\n",
      "--------------------------------------------------------------------------------\n",
      "mBART_large:\n",
      "-----------------\n",
      "Test BLEU:  34.681053161621094\n",
      "Training time:  31.333333333333332  mins\n",
      "Inference time:  8.115819756189982  mins\n"
     ]
    }
   ],
   "source": [
    "model = LmForTranslation.load_from_checkpoint('../models/iwslt_2014/es-en/mBART_large/training/checkpoints/'\n",
    "                                              'check-epoch=00-BLEU=36.47.ckpt')\n",
    "\n",
    "start_time = time.time()\n",
    "test_bleu_mbart_large = trainer.test(model)[0]['BLEU']\n",
    "training_time_mbart_large = 1880\n",
    "inference_time_mbart_large = time.time() - start_time\n",
    "print('mBART_large:')\n",
    "print('-----------------')\n",
    "print('Test BLEU: ', test_bleu_mbart_large)\n",
    "print('Training time: ', training_time_mbart_large / 60, ' mins')\n",
    "print('Inference time: ', inference_time_mbart_large / 60, ' mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 26 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0b50a221e041a8843bffb7182de318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BLEU': 32.785400390625}\n",
      "--------------------------------------------------------------------------------\n",
      "mBART_large_plus_adapter:\n",
      "-----------------\n",
      "Test BLEU:  32.785400390625\n",
      "Training time:  31.333333333333332  mins\n",
      "Inference time:  8.449832185109456  mins\n"
     ]
    }
   ],
   "source": [
    "model = LmForTranslation.load_from_checkpoint('../models/iwslt_2014/es-en/mBART_large_plus_adapter/training/'\n",
    "                                              'checkpoints/check-epoch=00-BLEU=34.48.ckpt')\n",
    "\n",
    "start_time = time.time()\n",
    "test_bleu_mbart_large_plus_adapter = trainer.test(model)[0]['BLEU']\n",
    "training_time_mbart_large_plus_adapter = 4258\n",
    "inference_time_mbart_large_plus_adapter = time.time() - start_time\n",
    "print('mBART_large_plus_adapter:')\n",
    "print('-----------------')\n",
    "print('Test BLEU: ', test_bleu_mbart_large_plus_adapter)\n",
    "print('Training time: ', training_time_mbart_large_plus_adapter / 60, ' mins')\n",
    "print('Inference time: ', inference_time_mbart_large_plus_adapter / 60, ' mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
