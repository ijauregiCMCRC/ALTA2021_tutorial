{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Translation notebook\n",
    "\n",
    "This is the notebook for translation\n",
    "\n",
    "(More descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TestTubeLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from src.translation_lightning_model import LmForTranslation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args ={\n",
    "    'train_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/train_pr',  # Path to training data\n",
    "    'validation_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/dev2010',  # Path to validation data\n",
    "    'test_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/test_joined',  # Path to test data\n",
    "    'src': 'en',  # Source language prefix\n",
    "    'tgt': 'es',  # Target language prefix\n",
    "    'max_src_len': 170,  # Maximum number of tokens in the source sentence\n",
    "    'max_tgt_len': 170,  # Maximum number of tokens in the target sentence\n",
    "    'save_dir': '../models/iwslt_2014/es-en/es_en_translation_bart_en_jupyter',  # Path to save the model and logs\n",
    "    'tokenizer': '../pretrained_lms/facebook-bart-base',  # Pretrained tokenizer\n",
    "    'model': '../pretrained_lms/facebook-bart-base',  # Pretrained model\n",
    "    'add_adapter': False,  # Include adapter training\n",
    "    'reduction_factor': 1,  # Adapter's reduction factor (>= 1)\n",
    "    'label_smoothing': 0.1, # Label smoothing \n",
    "    'epochs': 1,  # Number of epochs during training\n",
    "    'batch_size': 16,  # Batch size\n",
    "    'grad_accum': 1,  # Gradient accumulation\n",
    "    'lr': 0.00003,  # Training learning rate\n",
    "    'warmup': 500,  # Number of warmup steps\n",
    "    'weight_decay': 0.00003,  # Adam weight decay\n",
    "    'gpus': 1,  # Number of gpus. 0 for CPU\n",
    "    'precision': 32,  # Double precision (64), full precision (32) \n",
    "                      # or half precision (16). Can be used on CPU, GPU or TPUs.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:488: LightningDeprecationWarning: Argument `period` in `ModelCheckpoint` is deprecated in v1.3 and will be removed in v1.5. Please use `every_n_epochs` instead.\n",
      "  \"Argument `period` in `ModelCheckpoint` is deprecated in v1.3 and will be removed in v1.5.\"\n",
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:111: LightningDeprecationWarning: `Trainer(distributed_backend=dp)` has been deprecated and will be removed in v1.5. Use `Trainer(accelerator=dp)` instead.\n",
      "  f\"`Trainer(distributed_backend={distributed_backend})` has been deprecated and will be removed in v1.5.\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': './my_datasets/IWSLT_2014_TEDtalks/es-en/train_pr', 'validation': './my_datasets/IWSLT_2014_TEDtalks/es-en/dev2010', 'test': './my_datasets/IWSLT_2014_TEDtalks/es-en/test_joined'}\n",
      "{'train_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/train_pr', 'validation_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/dev2010', 'test_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/test_joined', 'src': 'en', 'tgt': 'es', 'max_src_len': 170, 'max_tgt_len': 170, 'save_dir': '../models/iwslt_2014/es-en/es_en_translation_bart_en_jupyter', 'tokenizer': '../pretrained_lms/facebook-bart-base', 'model': '../pretrained_lms/facebook-bart-base', 'add_adapter': False, 'reduction_factor': 1, 'label_smoothing': 0.1, 'epochs': 1, 'batch_size': 16, 'grad_accum': 1, 'lr': 3e-05, 'warmup': 500, 'weight_decay': 3e-05, 'gpus': 1, 'precision': 32, 'from_pretrained': None, 'dataset_size': 180850}\n"
     ]
    }
   ],
   "source": [
    "# Initialize with a seed\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# dataset size. Needed to compute number of steps for the lr scheduler\n",
    "args['dataset_size'] = sum(1 for line in open(args['train_data'] + '.' + args['src']))\n",
    "\n",
    "# Define PyTorch Lightning model\n",
    "model = LmForTranslation(args)\n",
    "# Include datasets\n",
    "model.hf_datasets = {'train': args['train_data'],\n",
    "                     'validation': args['validation_data'],\n",
    "                     'test': args['test_data']}\n",
    "print(model.hf_datasets)\n",
    "\n",
    "# Define logger\n",
    "logger = TestTubeLogger(\n",
    "    save_dir=args['save_dir'],\n",
    "    name='training',\n",
    "    version=0  # always use version=0\n",
    ")\n",
    "\n",
    "# Define checkpoint saver\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(args['save_dir'], \"training\", \"checkpoints\"),  # Dir path\n",
    "    filename='check-{epoch:02d}-{BLEU:.2f}',  # Filename\n",
    "    save_top_k=1,  # Maximum number of checkpoints to be saved\n",
    "    verbose=True,  # Verbose\n",
    "    monitor='BLEU',  # Checkpointing measurement (BLEU validation)\n",
    "    mode='max',      # Maximize measurement over the validation\n",
    "    period=1         # Save every epoch\n",
    ")\n",
    "\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Define lightning trainer\n",
    "trainer = pl.Trainer(gpus=args['gpus'], distributed_backend='dp' if torch.cuda.is_available() else None,\n",
    "                     track_grad_norm=-1,\n",
    "                     max_epochs=args['epochs'],\n",
    "                     max_steps=None,\n",
    "                     replace_sampler_ddp=False,\n",
    "                     accumulate_grad_batches=args['grad_accum'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     val_check_interval=1.0,\n",
    "                     num_sanity_val_steps=2,\n",
    "                     check_val_every_n_epoch=1,\n",
    "                     logger=logger,\n",
    "                     callbacks=checkpoint_callback,\n",
    "                     progress_bar_refresh_rate=10,\n",
    "                     precision=args['precision'],\n",
    "                     amp_backend='native', amp_level='O2',\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                         | Params\n",
      "-------------------------------------------------------\n",
      "0 | model | BartForConditionalGeneration | 139 M \n",
      "-------------------------------------------------------\n",
      "139 M     Trainable params\n",
      "0         Non-trainable params\n",
      "139 M     Total params\n",
      "557.682   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/train_pr', 'validation_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/dev2010', 'test_data': './my_datasets/IWSLT_2014_TEDtalks/es-en/test_joined', 'src': 'en', 'tgt': 'es', 'max_src_len': 170, 'max_tgt_len': 170, 'save_dir': '../models/iwslt_2014/es-en/es_en_translation_bart_en_jupyter', 'tokenizer': '../pretrained_lms/facebook-bart-base', 'model': '../pretrained_lms/facebook-bart-base', 'add_adapter': False, 'reduction_factor': 1, 'label_smoothing': 0.1, 'epochs': 1, 'batch_size': 16, 'grad_accum': 1, 'lr': 3e-05, 'warmup': 500, 'weight_decay': 3e-05, 'gpus': 1, 'precision': 32, 'from_pretrained': None, 'dataset_size': 180850}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38794bcbbd9a4bdf9023345c4b1b16a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 26 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 1024). Running this sequence through the model will result in indexing errors\n",
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 26 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514f3f81738149f7bcb2e33867a936f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 11280: BLEU reached 30.43455 (best 30.43455), saving model to \"/data/injaureg/Desktop/CMCRC/ALTA/ALTA2021_tutorial/models/iwslt_2014/es-en/es_en_translation_bart_en_jupyter/training/checkpoints/check-epoch=00-BLEU=30.43.ckpt\" as top 1\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/data/injaureg/Desktop/py36_alta/lib64/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 26 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1432443c306c4ed9b8508b2692def757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_val_loss': tensor(2.5985, device='cuda:0'), 'avg_accuracy': tensor(0.3291, device='cuda:0'), 'log': {'vloss': tensor(2.5985, device='cuda:0'), 'vaccuracy': tensor(0.3291, device='cuda:0')}, 'progress_bar': {'vloss': tensor(2.5985, device='cuda:0'), 'vaccuracy': tensor(0.3291, device='cuda:0')}}\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'BLEU': 29.53417205810547}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'BLEU': 29.53417205810547}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola, mi nombre es Inigo.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define PyTorch Lightning model\n",
    "model = LmForTranslation.load_from_checkpoint('../models/iwslt_2014/es-en/es_en_translation_bart_en_jupyter/'\n",
    "                                              'training/checkpoints/check-epoch=00-BLEU=30.43.ckpt')\n",
    "\n",
    "sentence = 'Hello my name is Inigo.'\n",
    "translation = model.translate_example(sentence)\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}