{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Machine Translation (NMT) model training\n",
    "\n",
    "In this section of the tutorial we show how quickly to train a\n",
    "state-of-the-art NMT model, by leveraging pretrained BART (Lewis et al., 2019) and mBART (multilingual BART)\n",
    "models available in [Huggingface](https://huggingface.co/). Additionally, we show how train a model using\n",
    "adapters from the [AdapterHub](https://adapterhub.ml/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setup\n",
    "#### (Only for  Google Colab Execution)\n",
    "\n",
    "If you are running the notebook in Google Colab, run the cell below to download the repository witht he required files to run the models and the requirements file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ijauregiCMCRC/ALTA2021_tutorial.git\n",
    "%cd ALTA2021_tutorial/translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install requirements\n",
    "__Note__: You may have to re-start the runtime environment in Google Colab after\n",
    "installing the requried packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Covers:\n",
    "- How to Train an evaluate a `tiny-bart` model.\n",
    "- Understand how to use the `pl.LightningModule` and`pl.Trainer`.\n",
    "- Understand the arguments required to define the model architecture and the model training.\n",
    "\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.getcwd()\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TestTubeLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# For dataset and pretrained model download\n",
    "import gdown\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom LightningModule\n",
    "from src.translation_lightning_model import LmForTranslation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset\n",
    "We use the [IWSLT 2014 TED Talks](https://sites.google.com/site/iwsltevaluation2014/home) transcripts\n",
    "English to Spanish translation dataset. This is common benchmark used by the NLP community for\n",
    "machine translation research.\n",
    "\n",
    "We provide a Google Drive link to download the dataset with minimum preprocessing (i.e. sentence alignment,\n",
    "remove sentences longer than 170 tokens) and our train (aprox. 180,000 sentences), dev (aprox. 900 sentences)  and test\n",
    "(aprox. 4,700 sentences) splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models folder\n",
    "!mkdir my_datasets\n",
    "# Download dataset from google drive\n",
    "dataset_link_drive = 'https://drive.google.com/uc?id=1MxrReEXbJPWa3OobANwfzak5rbs5kyNz'\n",
    "dataset_path = './my_datasets/IWSLT_2014_TEDtalks.zip'\n",
    "gdown.download(dataset_link_drive, dataset_path, quiet=False)\n",
    "!unzip './my_datasets/IWSLT_2014_TEDtalks.zip' -d './my_datasets/'\n",
    "!rm './my_datasets/IWSLT_2014_TEDtalks.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters\n",
    "The following hyperparameters define the architecture of the NMT model, the training, validation and test sets,\n",
    "and various aspects of the training job (e.g. number of epocjs, loss functions, learning rate, batch size...).\n",
    "\n",
    "__Note__: In this exercise we are using a `facebook/bart-base`\n",
    "[model from Huggingface](https://huggingface.co/facebook/bart-base). As the name indicates, this is\n",
    "a very small model (~500K parameters), and it doesn't have enough capacity to learn the translation task,\n",
    "but is a good model to test that the code works. Feel free to change the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args ={\n",
    "    'train_data': './my_datasets/IWSLT_2014/es-en/train',  # Path to training data\n",
    "    'validation_data': './my_datasets/IWSLT_2014/es-en/dev',  # Path to validation data\n",
    "    'test_data': './my_datasets/IWSLT_2014/es-en/test',  # Path to test data\n",
    "    'src': 'en',  # Source language prefix\n",
    "    'tgt': 'es',  # Target language prefix\n",
    "    'max_src_len': 170,  # Maximum number of tokens in the source sentence\n",
    "    'max_tgt_len': 170,  # Maximum number of tokens in the target sentence\n",
    "    'save_dir': './models/facebook_bart_base',  # Path to save the model and logs\n",
    "    'tokenizer': 'facebook/bart-base',  # Pretrained tokenizer\n",
    "    'model': 'facebook/bart-base',  # Pretrained model\n",
    "    'add_adapter': False,  # Include adapter training\n",
    "    'reduction_factor': 1,  # Adapter's reduction factor (>= 1)\n",
    "    'label_smoothing': 0.1, # Label smoothing \n",
    "    'epochs': 1,  # Number of epochs during training\n",
    "    'batch_size': 8,  # Batch size\n",
    "    'grad_accum': 1,  # Gradient accumulation\n",
    "    'lr': 0.00003,  # Training learning rate\n",
    "    'warmup': 500,  # Number of warmup steps\n",
    "    'weight_decay': 0.00003,  # Adam weight decay\n",
    "    'gpus': 1,  # Number of gpus. 0 for CPU\n",
    "    'precision': 32,  # Double precision (64), full precision (32) \n",
    "                      # or half precision (16). Can be used on CPU, GPU or TPUs.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Lightning module\n",
    "- We initialize the random, numpy, torch and cuda with the same seed.\n",
    "- We initialize our custom LightningModule (`LmForTranslation`).\n",
    "- We initialize the logger to capture training information.\n",
    "- We create a checkpointing callback to save the best model during training.\n",
    "- We define the Pytorch Lightning trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with a seed\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# dataset size. Needed to compute number of steps for the lr scheduler\n",
    "args['dataset_size'] = sum(1 for line in open(args['train_data'] + '.' + args['src']))\n",
    "\n",
    "# Define PyTorch Lightning model\n",
    "model = LmForTranslation(args)\n",
    "print(model.hf_datasets)\n",
    "\n",
    "# Define logger\n",
    "logger = TestTubeLogger(\n",
    "    save_dir=args['save_dir'],\n",
    "    name='training',\n",
    "    version=0  # always use version=0\n",
    ")\n",
    "\n",
    "# Define checkpoint saver\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(args['save_dir'], \"training\", \"checkpoints\"),  # Dir path\n",
    "    filename='check-{epoch:02d}-{BLEU:.2f}',  # Filename\n",
    "    save_top_k=1,    # Maximum number of checkpoints to be saved\n",
    "    verbose=True,    # Verbose\n",
    "    monitor='BLEU',  # Checkpointing measurement (BLEU validation)\n",
    "    mode='max',      # Maximize measurement over the validation\n",
    "    period=1         # Save every epoch\n",
    ")\n",
    "\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Define lightning trainer\n",
    "trainer = pl.Trainer(gpus=args['gpus'], distributed_backend='dp' if torch.cuda.is_available() else None,\n",
    "                     track_grad_norm=-1,\n",
    "                     max_epochs=args['epochs'],\n",
    "                     max_steps=None,\n",
    "                     replace_sampler_ddp=False,\n",
    "                     accumulate_grad_batches=args['grad_accum'],\n",
    "                     gradient_clip_val=1.0,\n",
    "                     val_check_interval=1.0,\n",
    "                     num_sanity_val_steps=2,\n",
    "                     check_val_every_n_epoch=1,\n",
    "                     logger=logger,\n",
    "                     callbacks=checkpoint_callback,\n",
    "                     progress_bar_refresh_rate=10,\n",
    "                     precision=args['precision'],\n",
    "                     amp_backend='native', amp_level='O2',\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "Simply call `trainer.fit()` with your lightning model and the training will start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Train model\n",
    "trainer.fit(model)\n",
    "print((time.time() - start_time)/60, ' mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n",
    "\n",
    "__Note__: `tiny-mbart` model cannot achieve a good BLEU score. Try to train other models, for more epochs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Comparing models\n",
    "- Compare different NMT models that have been previously trained by us with the same code and same dataset.\n",
    "- We have used the same hyperparameters except `tokenizer`, `model` and `add_adapter`.\n",
    "- The models have been trained on a Nvidia Quadro RTX 6000 (24GB RAM)\n",
    "\n",
    "### Our models:\n",
    "- __BART_base__:\n",
    "    - Tokenizer and model: `facebook/bart-base` ([Huggingface link](https://huggingface.co/facebook/bart-base))\n",
    "    - Add adapter: `False`\n",
    "- __BART_base_with_adapter__:\n",
    "    - Tokenizer and model: `facebook/bart-base` ([Huggingface link](https://huggingface.co/facebook/bart-base))\n",
    "    - Add adapter: `True`\n",
    "- __mBART_large_with_adapter__:\n",
    "    - Tokenizer and model: `facebook/mbart-large-cc25` ([Huggingface link](https://huggingface.co/facebook/mbart-large-cc25))\n",
    "    - Add adapter: `True`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models folder\n",
    "!mkdir models\n",
    "# Download them from google drive\n",
    "# BART_base\n",
    "bart_base_url = 'https://drive.google.com/uc?id=1_VA85J5OOf3PltRqhbVLDuDNBtJ-9vOR'\n",
    "bart_base_out = './models/bart_base.zip'\n",
    "gdown.download(bart_base_url, bart_base_out, quiet=False)\n",
    "!unzip './models/bart_base.zip' -d './models/'\n",
    "!rm './models/bart_base.zip'\n",
    "# BART_base_with_adapter\n",
    "bart_base_with_adapter_url = 'https://drive.google.com/uc?id=1Rojznogzr6cMGmi3wt1BeTaputmf6jv4'\n",
    "bart_base_with_adapter_out = './models/bart_base_with_adapter.zip'\n",
    "gdown.download(bart_base_with_adapter_url, bart_base_with_adapter_out, quiet=False)\n",
    "!unzip './models/bart_base_with_adapter.zip' -d './models/'\n",
    "!rm './models/bart_base_with_adapter.zip'\n",
    "# mBART_large_with_adapter\n",
    "mbart_large_wa_url = 'https://drive.google.com/uc?id=1tCGSk021m8aMkYEd7tm_j-Hp2fYwY5e_'\n",
    "mbart_large_wa_out = './models/mbart_large_wa.zip'\n",
    "gdown.download(mbart_large_wa_url, mbart_large_wa_out, quiet=False)\n",
    "!unzip './models/mbart_large_wa.zip' -d './models/'\n",
    "!rm './models/mbart_large_wa.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English sentence example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence by Alan Turing\n",
    "sentence = 'Sometimes it is the people no one can imagine anything of who do the things no one can imagine.'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run test and inference over example\n",
    "\n",
    "#### BART_base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading model...')\n",
    "model = LmForTranslation.load_from_checkpoint('./models/BART_base/training/checkpoints/'\n",
    "                                              'check-epoch=00-BLEU=32.51.ckpt')\n",
    "# Update dataset path (if required)\n",
    "model.hf_datasets = {'train': args['train_data'],\n",
    "                     'validation': args['validation_data'],\n",
    "                     'test': args['test_data']}\n",
    "tp_bart_base, ntp_bart_base = model.num_parameters()\n",
    "start_time = time.time()\n",
    "test_bleu_bart_base = trainer.test(model)[0]['BLEU']\n",
    "training_time_bart_base = 2972 / 60 # Note that the training time is harcoded (this is the time it took as to train this model in a Nvidia Quadro RTX 6000\n",
    "inference_time_bart_base = (time.time() - start_time) / 60\n",
    "translation_example_bart_base = model.translate_example(sentence)\n",
    "print('BART_base:')\n",
    "print('-----------------')\n",
    "print('Trainable parameters: ', tp_bart_base)\n",
    "print('Non-trainable parameters: ', ntp_bart_base)\n",
    "print('Total parameters: ', tp_bart_base + ntp_bart_base)\n",
    "print('-----------------')\n",
    "print('Test BLEU: ', test_bleu_bart_base)\n",
    "print('Training time: ', training_time_bart_base, ' mins')\n",
    "print('Inference time: ', inference_time_bart_base, ' mins')\n",
    "print('Translation example-> ', translation_example_bart_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### BART_base_with_adapter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Loading model...')\n",
    "model = LmForTranslation.load_from_checkpoint('./models/BART_base_with_adapter/training/checkpoints/'\n",
    "                                              'check-epoch=00-BLEU=20.97.ckpt')\n",
    "# Update dataset path (if required)\n",
    "model.hf_datasets = {'train': args['train_data'],\n",
    "                     'validation': args['validation_data'],\n",
    "                     'test': args['test_data']}\n",
    "tp_bart_base_plus_adapter, ntp_bart_base_plus_adapter = model.num_parameters()\n",
    "start_time = time.time()\n",
    "test_bleu_bart_base_plus_adapter = trainer.test(model)[0]['BLEU']\n",
    "training_time_bart_base_plus_adapter = 1880 / 60 # Note that the training time is harcoded (this is the time it took as to train this model in a Nvidia Quadro RTX 6000\n",
    "inference_time_bart_base_plus_adapter = (time.time() - start_time) / 60\n",
    "translation_example_bart_base_plus_adapter = model.translate_example(sentence)\n",
    "print('BART_base_plus_adapter:')\n",
    "print('-----------------')\n",
    "print('Trainable parameters: ', tp_bart_base_plus_adapter)\n",
    "print('Non-trainable parameters: ', ntp_bart_base_plus_adapter)\n",
    "print('Total parameters: ', tp_bart_base_plus_adapter + ntp_bart_base_plus_adapter)\n",
    "print('-----------------')\n",
    "print('Test BLEU: ', test_bleu_bart_base_plus_adapter)\n",
    "print('Training time: ', training_time_bart_base_plus_adapter, ' mins')\n",
    "print('Inference time: ', inference_time_bart_base_plus_adapter, ' mins')\n",
    "print('Translation example-> ', translation_example_bart_base_plus_adapter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### mBART_large_with_adapter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading model...')\n",
    "model = LmForTranslation.load_from_checkpoint('./models/mBART_large_plus_adapter/training/'\n",
    "                                              'checkpoints/check-epoch=00-BLEU=34.48.ckpt')\n",
    "# Update dataset path (if required)\n",
    "model.hf_datasets = {'train': args['train_data'],\n",
    "                     'validation': args['validation_data'],\n",
    "                     'test': args['test_data']}\n",
    "tp_mbart_large_plus_adapter, ntp_mbart_large_plus_adapter = model.num_parameters()\n",
    "start_time = time.time()\n",
    "test_bleu_mbart_large_plus_adapter = trainer.test(model)[0]['BLEU']\n",
    "training_time_mbart_large_plus_adapter = 4258 / 60  # Note that the training time is harcoded (this is the time it took as to train this model in a Nvidia Quadro RTX 6000\n",
    "inference_time_mbart_large_plus_adapter = (time.time() - start_time) / 60\n",
    "translation_example_mbart_large_plus_adapter = model.translate_example(sentence)\n",
    "print('mBART_large_plus_adapter:')\n",
    "print('-----------------')\n",
    "print('Trainable parameters: ', tp_mbart_large_plus_adapter)\n",
    "print('Non-trainable parameters: ', ntp_mbart_large_plus_adapter)\n",
    "print('Total parameters: ', tp_mbart_large_plus_adapter + ntp_mbart_large_plus_adapter)\n",
    "print('-----------------')\n",
    "print('Test BLEU: ', test_bleu_mbart_large_plus_adapter)\n",
    "print('Training time: ', training_time_mbart_large_plus_adapter, ' mins')\n",
    "print('Inference time: ', inference_time_mbart_large_plus_adapter, ' mins')\n",
    "print('Translation example-> ', translation_example_mbart_large_plus_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ploting results\n",
    "\n",
    "Lets draw some basic barplots to compare the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_names = ['BART_base', 'BART_base (wa)', 'mBART_large (wa)']\n",
    "colors = ['black', 'red', 'green']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Scores\n",
    "plt.bar(x_names,\n",
    "       [test_bleu_bart_base, test_bleu_bart_base_plus_adapter, test_bleu_mbart_large_plus_adapter],\n",
    "       color=colors)\n",
    "plt.ylim((30,36))\n",
    "plt.ylabel('BLEU')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title('Test set evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time vs inference time\n",
    "fig, axs = plt.subplots(1,2, figsize=(15,5))\n",
    "axs[0].bar(x_names,\n",
    "       [training_time_bart_base, training_time_bart_base_plus_adapter, training_time_mbart_large_plus_adapter],\n",
    "       color=colors)\n",
    "axs[0].set_ylabel('mins')\n",
    "axs[0].set_title('training time')\n",
    "axs[1].bar(x_names,\n",
    "       [inference_time_bart_base, inference_time_bart_base_plus_adapter, inference_time_mbart_large_plus_adapter],\n",
    "       color=colors)\n",
    "axs[1].set_ylabel('mins')\n",
    "axs[1].set_title('inference time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "axs[0].bar(x_names,\n",
    "       [tp_bart_base/1000000, tp_bart_base_plus_adapter/1000000, tp_mbart_large_plus_adapter/1000000],\n",
    "       color=colors)\n",
    "axs[0].set_ylabel('Million')\n",
    "axs[0].set_title('# trainable parameters')\n",
    "axs[1].bar(x_names,\n",
    "       [ntp_bart_base/1000000, ntp_bart_base_plus_adapter/1000000, ntp_mbart_large_plus_adapter/1000000],\n",
    "       color=colors)\n",
    "axs[1].set_ylabel('Million')\n",
    "axs[1].set_title('# non-trainable parameters')\n",
    "axs[2].bar(x_names,\n",
    "       [(tp_bart_base + ntp_bart_base)/1000000, (tp_bart_base_plus_adapter + ntp_bart_base_plus_adapter)/1000000, (tp_mbart_large_plus_adapter + ntp_mbart_large_plus_adapter)//1000000],\n",
    "       color=colors)\n",
    "axs[2].set_ylabel('Million')\n",
    "axs[2].set_title('# total parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare translation examples\n",
    "print('BART_base:')\n",
    "print(' -> ', translation_example_bart_base)\n",
    "print('BART_base (wa):')\n",
    "print(' -> ', translation_example_bart_base_plus_adapter)\n",
    "print('mBART_large (wa):')\n",
    "print(' -> ', translation_example_mbart_large_plus_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Well done!\n",
    "\n",
    "You have reach the end of the notebook. Now feel free to change and play with it as much as you like\n",
    "(hyperparameters, language pairs, models...). Have fun training your own NMT models!"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}